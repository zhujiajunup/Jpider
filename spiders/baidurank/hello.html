

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Settings &mdash; Scrapy 1.3.3 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  

  
    <link rel="stylesheet" href="https://media.readthedocs.org/css/sphinx_rtd_theme.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="Scrapy 1.3.3 documentation" href="../index.html"/>
        <link rel="next" title="Exceptions" href="exceptions.html"/>
        <link rel="prev" title="Link Extractors" href="link-extractors.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>


<!-- RTD Extra Head -->

<!-- 
Always link to the latest version, as canonical.
http://docs.readthedocs.org/en/latest/canonical.html
-->
<link rel="canonical" href="http://doc.scrapy.org/en/latest/topics/settings.html" />

<link rel="stylesheet" href="https://media.readthedocs.org/css/readthedocs-doc-embed.css" type="text/css" />

<script type="text/javascript" src="../_static/readthedocs-data.js"></script>

<!-- Add page-specific data, which must exist in the page js, not global -->
<script type="text/javascript">
READTHEDOCS_DATA['page'] = 'topics/settings' 		
READTHEDOCS_DATA['source_suffix'] = '.rst'
</script>

<script type="text/javascript" src="../_static/readthedocs-dynamic-include.js"></script>

<!-- end RTD <extrahead> --></head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Scrapy
          

          
          </a>

          
            
            
            
              <div class="version">
                latest
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">First steps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/overview.html">Scrapy at a glance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/install.html">Installation guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/tutorial.html">Scrapy Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/examples.html">Examples</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="commands.html">Command line tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="spiders.html">Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="selectors.html">Selectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="items.html">Items</a></li>
<li class="toctree-l1"><a class="reference internal" href="loaders.html">Item Loaders</a></li>
<li class="toctree-l1"><a class="reference internal" href="shell.html">Scrapy shell</a></li>
<li class="toctree-l1"><a class="reference internal" href="item-pipeline.html">Item Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="feed-exports.html">Feed exports</a></li>
<li class="toctree-l1"><a class="reference internal" href="request-response.html">Requests and Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="link-extractors.html">Link Extractors</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Settings</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#designating-the-settings">Designating the settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="#populating-the-settings">Populating the settings</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#command-line-options">1. Command line options</a></li>
<li class="toctree-l3"><a class="reference internal" href="#settings-per-spider">2. Settings per-spider</a></li>
<li class="toctree-l3"><a class="reference internal" href="#project-settings-module">3. Project settings module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#default-settings-per-command">4. Default settings per-command</a></li>
<li class="toctree-l3"><a class="reference internal" href="#default-global-settings">5. Default global settings</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-access-settings">How to access settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rationale-for-setting-names">Rationale for setting names</a></li>
<li class="toctree-l2"><a class="reference internal" href="#built-in-settings-reference">Built-in settings reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#aws-access-key-id">AWS_ACCESS_KEY_ID</a></li>
<li class="toctree-l3"><a class="reference internal" href="#aws-secret-access-key">AWS_SECRET_ACCESS_KEY</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bot-name">BOT_NAME</a></li>
<li class="toctree-l3"><a class="reference internal" href="#concurrent-items">CONCURRENT_ITEMS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#concurrent-requests">CONCURRENT_REQUESTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#concurrent-requests-per-domain">CONCURRENT_REQUESTS_PER_DOMAIN</a></li>
<li class="toctree-l3"><a class="reference internal" href="#concurrent-requests-per-ip">CONCURRENT_REQUESTS_PER_IP</a></li>
<li class="toctree-l3"><a class="reference internal" href="#default-item-class">DEFAULT_ITEM_CLASS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#default-request-headers">DEFAULT_REQUEST_HEADERS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#depth-limit">DEPTH_LIMIT</a></li>
<li class="toctree-l3"><a class="reference internal" href="#depth-priority">DEPTH_PRIORITY</a></li>
<li class="toctree-l3"><a class="reference internal" href="#depth-stats">DEPTH_STATS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#depth-stats-verbose">DEPTH_STATS_VERBOSE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dnscache-enabled">DNSCACHE_ENABLED</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dnscache-size">DNSCACHE_SIZE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dns-timeout">DNS_TIMEOUT</a></li>
<li class="toctree-l3"><a class="reference internal" href="#downloader">DOWNLOADER</a></li>
<li class="toctree-l3"><a class="reference internal" href="#downloader-httpclientfactory">DOWNLOADER_HTTPCLIENTFACTORY</a></li>
<li class="toctree-l3"><a class="reference internal" href="#downloader-clientcontextfactory">DOWNLOADER_CLIENTCONTEXTFACTORY</a></li>
<li class="toctree-l3"><a class="reference internal" href="#downloader-client-tls-method">DOWNLOADER_CLIENT_TLS_METHOD</a></li>
<li class="toctree-l3"><a class="reference internal" href="#downloader-middlewares">DOWNLOADER_MIDDLEWARES</a></li>
<li class="toctree-l3"><a class="reference internal" href="#downloader-middlewares-base">DOWNLOADER_MIDDLEWARES_BASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#downloader-stats">DOWNLOADER_STATS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#download-delay">DOWNLOAD_DELAY</a></li>
<li class="toctree-l3"><a class="reference internal" href="#download-handlers">DOWNLOAD_HANDLERS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#download-handlers-base">DOWNLOAD_HANDLERS_BASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#download-timeout">DOWNLOAD_TIMEOUT</a></li>
<li class="toctree-l3"><a class="reference internal" href="#download-maxsize">DOWNLOAD_MAXSIZE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#download-warnsize">DOWNLOAD_WARNSIZE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dupefilter-class">DUPEFILTER_CLASS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dupefilter-debug">DUPEFILTER_DEBUG</a></li>
<li class="toctree-l3"><a class="reference internal" href="#editor">EDITOR</a></li>
<li class="toctree-l3"><a class="reference internal" href="#extensions">EXTENSIONS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#extensions-base">EXTENSIONS_BASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#feed-tempdir">FEED_TEMPDIR</a></li>
<li class="toctree-l3"><a class="reference internal" href="#item-pipelines">ITEM_PIPELINES</a></li>
<li class="toctree-l3"><a class="reference internal" href="#item-pipelines-base">ITEM_PIPELINES_BASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#log-enabled">LOG_ENABLED</a></li>
<li class="toctree-l3"><a class="reference internal" href="#log-encoding">LOG_ENCODING</a></li>
<li class="toctree-l3"><a class="reference internal" href="#log-file">LOG_FILE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#log-format">LOG_FORMAT</a></li>
<li class="toctree-l3"><a class="reference internal" href="#log-dateformat">LOG_DATEFORMAT</a></li>
<li class="toctree-l3"><a class="reference internal" href="#log-level">LOG_LEVEL</a></li>
<li class="toctree-l3"><a class="reference internal" href="#log-stdout">LOG_STDOUT</a></li>
<li class="toctree-l3"><a class="reference internal" href="#log-short-names">LOG_SHORT_NAMES</a></li>
<li class="toctree-l3"><a class="reference internal" href="#memdebug-enabled">MEMDEBUG_ENABLED</a></li>
<li class="toctree-l3"><a class="reference internal" href="#memdebug-notify">MEMDEBUG_NOTIFY</a></li>
<li class="toctree-l3"><a class="reference internal" href="#memusage-enabled">MEMUSAGE_ENABLED</a></li>
<li class="toctree-l3"><a class="reference internal" href="#memusage-limit-mb">MEMUSAGE_LIMIT_MB</a></li>
<li class="toctree-l3"><a class="reference internal" href="#memusage-check-interval-seconds">MEMUSAGE_CHECK_INTERVAL_SECONDS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#memusage-notify-mail">MEMUSAGE_NOTIFY_MAIL</a></li>
<li class="toctree-l3"><a class="reference internal" href="#memusage-report">MEMUSAGE_REPORT</a></li>
<li class="toctree-l3"><a class="reference internal" href="#memusage-warning-mb">MEMUSAGE_WARNING_MB</a></li>
<li class="toctree-l3"><a class="reference internal" href="#newspider-module">NEWSPIDER_MODULE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#randomize-download-delay">RANDOMIZE_DOWNLOAD_DELAY</a></li>
<li class="toctree-l3"><a class="reference internal" href="#reactor-threadpool-maxsize">REACTOR_THREADPOOL_MAXSIZE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#redirect-max-times">REDIRECT_MAX_TIMES</a></li>
<li class="toctree-l3"><a class="reference internal" href="#redirect-priority-adjust">REDIRECT_PRIORITY_ADJUST</a></li>
<li class="toctree-l3"><a class="reference internal" href="#retry-priority-adjust">RETRY_PRIORITY_ADJUST</a></li>
<li class="toctree-l3"><a class="reference internal" href="#robotstxt-obey">ROBOTSTXT_OBEY</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scheduler">SCHEDULER</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scheduler-debug">SCHEDULER_DEBUG</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scheduler-disk-queue">SCHEDULER_DISK_QUEUE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scheduler-memory-queue">SCHEDULER_MEMORY_QUEUE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scheduler-priority-queue">SCHEDULER_PRIORITY_QUEUE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spider-contracts">SPIDER_CONTRACTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spider-contracts-base">SPIDER_CONTRACTS_BASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spider-loader-class">SPIDER_LOADER_CLASS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spider-loader-warn-only">SPIDER_LOADER_WARN_ONLY</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spider-middlewares">SPIDER_MIDDLEWARES</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spider-middlewares-base">SPIDER_MIDDLEWARES_BASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spider-modules">SPIDER_MODULES</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stats-class">STATS_CLASS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stats-dump">STATS_DUMP</a></li>
<li class="toctree-l3"><a class="reference internal" href="#statsmailer-rcpts">STATSMAILER_RCPTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#telnetconsole-enabled">TELNETCONSOLE_ENABLED</a></li>
<li class="toctree-l3"><a class="reference internal" href="#telnetconsole-port">TELNETCONSOLE_PORT</a></li>
<li class="toctree-l3"><a class="reference internal" href="#templates-dir">TEMPLATES_DIR</a></li>
<li class="toctree-l3"><a class="reference internal" href="#urllength-limit">URLLENGTH_LIMIT</a></li>
<li class="toctree-l3"><a class="reference internal" href="#user-agent">USER_AGENT</a></li>
<li class="toctree-l3"><a class="reference internal" href="#settings-documented-elsewhere">Settings documented elsewhere:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="exceptions.html">Exceptions</a></li>
</ul>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="stats.html">Stats Collection</a></li>
<li class="toctree-l1"><a class="reference internal" href="email.html">Sending e-mail</a></li>
<li class="toctree-l1"><a class="reference internal" href="telnetconsole.html">Telnet Console</a></li>
<li class="toctree-l1"><a class="reference internal" href="webservice.html">Web Service</a></li>
</ul>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="debug.html">Debugging Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="contracts.html">Spiders Contracts</a></li>
<li class="toctree-l1"><a class="reference internal" href="practices.html">Common Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="broad-crawls.html">Broad Crawls</a></li>
<li class="toctree-l1"><a class="reference internal" href="firefox.html">Using Firefox for scraping</a></li>
<li class="toctree-l1"><a class="reference internal" href="firebug.html">Using Firebug for scraping</a></li>
<li class="toctree-l1"><a class="reference internal" href="leaks.html">Debugging memory leaks</a></li>
<li class="toctree-l1"><a class="reference internal" href="media-pipeline.html">Downloading and processing files and images</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">Deploying Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="autothrottle.html">AutoThrottle extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="jobs.html">Jobs: pausing and resuming crawls</a></li>
</ul>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="downloader-middleware.html">Downloader Middleware</a></li>
<li class="toctree-l1"><a class="reference internal" href="spider-middleware.html">Spider Middleware</a></li>
<li class="toctree-l1"><a class="reference internal" href="extensions.html">Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">Core API</a></li>
<li class="toctree-l1"><a class="reference internal" href="signals.html">Signals</a></li>
<li class="toctree-l1"><a class="reference internal" href="exporters.html">Item Exporters</a></li>
</ul>
<p class="caption"><span class="caption-text">All the rest</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../news.html">Release notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing to Scrapy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../versioning.html">Versioning and API Stability</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Scrapy</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Settings</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/scrapy/scrapy/blob/1.3/docs/topics/settings.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="settings">
<span id="topics-settings"></span><h1>Settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h1>
<p>The Scrapy settings allows you to customize the behaviour of all Scrapy
components, including the core, extensions, pipelines and spiders themselves.</p>
<p>The infrastructure of the settings provides a global namespace of key-value mappings
that the code can use to pull configuration values from. The settings can be
populated through different mechanisms, which are described below.</p>
<p>The settings are also the mechanism for selecting the currently active Scrapy
project (in case you have many).</p>
<p>For a list of available built-in settings see: <a class="reference internal" href="#topics-settings-ref"><span class="std std-ref">Built-in settings reference</span></a>.</p>
<div class="section" id="designating-the-settings">
<span id="topics-settings-module-envvar"></span><h2>Designating the settings<a class="headerlink" href="#designating-the-settings" title="Permalink to this headline">¶</a></h2>
<p>When you use Scrapy, you have to tell it which settings you&#8217;re using. You can
do this by using an environment variable, <code class="docutils literal"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code>.</p>
<p>The value of <code class="docutils literal"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code> should be in Python path syntax, e.g.
<code class="docutils literal"><span class="pre">myproject.settings</span></code>. Note that the settings module should be on the
Python <a class="reference external" href="https://docs.python.org/2/tutorial/modules.html#the-module-search-path">import search path</a>.</p>
</div>
<div class="section" id="populating-the-settings">
<h2>Populating the settings<a class="headerlink" href="#populating-the-settings" title="Permalink to this headline">¶</a></h2>
<p>Settings can be populated using different mechanisms, each of which having a
different precedence. Here is the list of them in decreasing order of
precedence:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Command line options (most precedence)</li>
<li>Settings per-spider</li>
<li>Project settings module</li>
<li>Default settings per-command</li>
<li>Default global settings (less precedence)</li>
</ol>
</div></blockquote>
<p>The population of these settings sources is taken care of internally, but a
manual handling is possible using API calls. See the
<a class="reference internal" href="api.html#topics-api-settings"><span class="std std-ref">Settings API</span></a> topic for reference.</p>
<p>These mechanisms are described in more detail below.</p>
<div class="section" id="command-line-options">
<h3>1. Command line options<a class="headerlink" href="#command-line-options" title="Permalink to this headline">¶</a></h3>
<p>Arguments provided by the command line are the ones that take most precedence,
overriding any other options. You can explicitly override one (or more)
settings using the <code class="docutils literal"><span class="pre">-s</span></code> (or <code class="docutils literal"><span class="pre">--set</span></code>) command line option.</p>
<p>Example:</p>
<div class="highlight-sh"><div class="highlight"><pre><span></span>scrapy crawl myspider -s <span class="nv">LOG_FILE</span><span class="o">=</span>scrapy.log
</pre></div>
</div>
</div>
<div class="section" id="settings-per-spider">
<h3>2. Settings per-spider<a class="headerlink" href="#settings-per-spider" title="Permalink to this headline">¶</a></h3>
<p>Spiders (See the <a class="reference internal" href="spiders.html#topics-spiders"><span class="std std-ref">Spiders</span></a> chapter for reference) can define their
own settings that will take precedence and override the project ones. They can
do so by setting their <a class="reference internal" href="spiders.html#scrapy.spiders.Spider.custom_settings" title="scrapy.spiders.Spider.custom_settings"><code class="xref py py-attr docutils literal"><span class="pre">custom_settings</span></code></a> attribute:</p>
<div class="highlight-sh"><div class="highlight"><pre><span></span>class MySpider<span class="o">(</span>scrapy.Spider<span class="o">)</span>:
    <span class="nv">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>

    <span class="nv">custom_settings</span> <span class="o">=</span> <span class="o">{</span>
        <span class="s1">&#39;SOME_SETTING&#39;</span>: <span class="s1">&#39;some value&#39;</span>,
    <span class="o">}</span>
</pre></div>
</div>
</div>
<div class="section" id="project-settings-module">
<h3>3. Project settings module<a class="headerlink" href="#project-settings-module" title="Permalink to this headline">¶</a></h3>
<p>The project settings module is the standard configuration file for your Scrapy
project, it&#8217;s where most of your custom settings will be populated. For a
standard Scrapy project, this means you&#8217;ll be adding or changing the settings
in the <code class="docutils literal"><span class="pre">settings.py</span></code> file created for your project.</p>
</div>
<div class="section" id="default-settings-per-command">
<h3>4. Default settings per-command<a class="headerlink" href="#default-settings-per-command" title="Permalink to this headline">¶</a></h3>
<p>Each <a class="reference internal" href="commands.html"><span class="doc">Scrapy tool</span></a> command can have its own default
settings, which override the global default settings. Those custom command
settings are specified in the <code class="docutils literal"><span class="pre">default_settings</span></code> attribute of the command
class.</p>
</div>
<div class="section" id="default-global-settings">
<h3>5. Default global settings<a class="headerlink" href="#default-global-settings" title="Permalink to this headline">¶</a></h3>
<p>The global defaults are located in the <code class="docutils literal"><span class="pre">scrapy.settings.default_settings</span></code>
module and documented in the <a class="reference internal" href="#topics-settings-ref"><span class="std std-ref">Built-in settings reference</span></a> section.</p>
</div>
</div>
<div class="section" id="how-to-access-settings">
<h2>How to access settings<a class="headerlink" href="#how-to-access-settings" title="Permalink to this headline">¶</a></h2>
<p>In a spider, the settings are available through <code class="docutils literal"><span class="pre">self.settings</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://example.com&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Existing settings: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">attributes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <code class="docutils literal"><span class="pre">settings</span></code> attribute is set in the base Spider class after the spider
is initialized.  If you want to use the settings before the initialization
(e.g., in your spider&#8217;s <code class="docutils literal"><span class="pre">__init__()</span></code> method), you&#8217;ll need to override the
<a class="reference internal" href="spiders.html#scrapy.spiders.Spider.from_crawler" title="scrapy.spiders.Spider.from_crawler"><code class="xref py py-meth docutils literal"><span class="pre">from_crawler()</span></code></a> method.</p>
</div>
<p>Settings can be accessed through the <a class="reference internal" href="api.html#scrapy.crawler.Crawler.settings" title="scrapy.crawler.Crawler.settings"><code class="xref py py-attr docutils literal"><span class="pre">scrapy.crawler.Crawler.settings</span></code></a>
attribute of the Crawler that is passed to <code class="docutils literal"><span class="pre">from_crawler</span></code> method in
extensions, middlewares and item pipelines:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyExtension</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_is_enabled</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">log_is_enabled</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;log is enabled!&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="n">settings</span> <span class="o">=</span> <span class="n">crawler</span><span class="o">.</span><span class="n">settings</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">getbool</span><span class="p">(</span><span class="s1">&#39;LOG_ENABLED&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>The settings object can be used like a dict (e.g.,
<code class="docutils literal"><span class="pre">settings['LOG_ENABLED']</span></code>), but it&#8217;s usually preferred to extract the setting
in the format you need it to avoid type errors, using one of the methods
provided by the <a class="reference internal" href="api.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> API.</p>
</div>
<div class="section" id="rationale-for-setting-names">
<h2>Rationale for setting names<a class="headerlink" href="#rationale-for-setting-names" title="Permalink to this headline">¶</a></h2>
<p>Setting names are usually prefixed with the component that they configure. For
example, proper setting names for a fictional robots.txt extension would be
<code class="docutils literal"><span class="pre">ROBOTSTXT_ENABLED</span></code>, <code class="docutils literal"><span class="pre">ROBOTSTXT_OBEY</span></code>, <code class="docutils literal"><span class="pre">ROBOTSTXT_CACHEDIR</span></code>, etc.</p>
</div>
<div class="section" id="built-in-settings-reference">
<span id="topics-settings-ref"></span><h2>Built-in settings reference<a class="headerlink" href="#built-in-settings-reference" title="Permalink to this headline">¶</a></h2>
<p>Here&#8217;s a list of all available Scrapy settings, in alphabetical order, along
with their default values and the scope where they apply.</p>
<p>The scope, where available, shows where the setting is being used, if it&#8217;s tied
to any particular component. In that case the module of that component will be
shown, typically an extension, middleware or pipeline. It also means that the
component must be enabled in order for the setting to have any effect.</p>
<div class="section" id="aws-access-key-id">
<span id="std:setting-AWS_ACCESS_KEY_ID"></span><h3>AWS_ACCESS_KEY_ID<a class="headerlink" href="#aws-access-key-id" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>The AWS access key used by code that requires access to <a class="reference external" href="https://aws.amazon.com/">Amazon Web services</a>,
such as the <a class="reference internal" href="feed-exports.html#topics-feed-storage-s3"><span class="std std-ref">S3 feed storage backend</span></a>.</p>
</div>
<div class="section" id="aws-secret-access-key">
<span id="std:setting-AWS_SECRET_ACCESS_KEY"></span><h3>AWS_SECRET_ACCESS_KEY<a class="headerlink" href="#aws-secret-access-key" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>The AWS secret key used by code that requires access to <a class="reference external" href="https://aws.amazon.com/">Amazon Web services</a>,
such as the <a class="reference internal" href="feed-exports.html#topics-feed-storage-s3"><span class="std std-ref">S3 feed storage backend</span></a>.</p>
</div>
<div class="section" id="bot-name">
<span id="std:setting-BOT_NAME"></span><h3>BOT_NAME<a class="headerlink" href="#bot-name" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'scrapybot'</span></code></p>
<p>The name of the bot implemented by this Scrapy project (also known as the
project name). This will be used to construct the User-Agent by default, and
also for logging.</p>
<p>It&#8217;s automatically populated with your project name when you create your
project with the <a class="reference internal" href="commands.html#std:command-startproject"><code class="xref std std-command docutils literal"><span class="pre">startproject</span></code></a> command.</p>
</div>
<div class="section" id="concurrent-items">
<span id="std:setting-CONCURRENT_ITEMS"></span><h3>CONCURRENT_ITEMS<a class="headerlink" href="#concurrent-items" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">100</span></code></p>
<p>Maximum number of concurrent items (per response) to process in parallel in the
Item Processor (also known as the <a class="reference internal" href="item-pipeline.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>).</p>
</div>
<div class="section" id="concurrent-requests">
<span id="std:setting-CONCURRENT_REQUESTS"></span><h3>CONCURRENT_REQUESTS<a class="headerlink" href="#concurrent-requests" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">16</span></code></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed by the Scrapy downloader.</p>
</div>
<div class="section" id="concurrent-requests-per-domain">
<span id="std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"></span><h3>CONCURRENT_REQUESTS_PER_DOMAIN<a class="headerlink" href="#concurrent-requests-per-domain" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">8</span></code></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed to any single domain.</p>
<p>See also: <a class="reference internal" href="autothrottle.html#topics-autothrottle"><span class="std std-ref">AutoThrottle extension</span></a> and its
<a class="reference internal" href="autothrottle.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a> option.</p>
</div>
<div class="section" id="concurrent-requests-per-ip">
<span id="std:setting-CONCURRENT_REQUESTS_PER_IP"></span><h3>CONCURRENT_REQUESTS_PER_IP<a class="headerlink" href="#concurrent-requests-per-ip" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed to any single IP. If non-zero, the
<a class="reference internal" href="#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> setting is ignored, and this one is
used instead. In other words, concurrency limits will be applied per IP, not
per domain.</p>
<p>This setting also affects <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a> and
<a class="reference internal" href="autothrottle.html#topics-autothrottle"><span class="std std-ref">AutoThrottle extension</span></a>: if <a class="reference internal" href="#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>
is non-zero, download delay is enforced per IP, not per domain.</p>
</div>
<div class="section" id="default-item-class">
<span id="std:setting-DEFAULT_ITEM_CLASS"></span><h3>DEFAULT_ITEM_CLASS<a class="headerlink" href="#default-item-class" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.item.Item'</span></code></p>
<p>The default class that will be used for instantiating items in the <a class="reference internal" href="shell.html#topics-shell"><span class="std std-ref">the
Scrapy shell</span></a>.</p>
</div>
<div class="section" id="default-request-headers">
<span id="std:setting-DEFAULT_REQUEST_HEADERS"></span><h3>DEFAULT_REQUEST_HEADERS<a class="headerlink" href="#default-request-headers" title="Permalink to this headline">¶</a></h3>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;Accept&#39;</span><span class="p">:</span> <span class="s1">&#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Accept-Language&#39;</span><span class="p">:</span> <span class="s1">&#39;en&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The default headers used for Scrapy HTTP Requests. They&#8217;re populated in the
<a class="reference internal" href="downloader-middleware.html#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware" title="scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware"><code class="xref py py-class docutils literal"><span class="pre">DefaultHeadersMiddleware</span></code></a>.</p>
</div>
<div class="section" id="depth-limit">
<span id="std:setting-DEPTH_LIMIT"></span><h3>DEPTH_LIMIT<a class="headerlink" href="#depth-limit" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>The maximum depth that will be allowed to crawl for any site. If zero, no limit
will be imposed.</p>
</div>
<div class="section" id="depth-priority">
<span id="std:setting-DEPTH_PRIORITY"></span><h3>DEPTH_PRIORITY<a class="headerlink" href="#depth-priority" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>An integer that is used to adjust the request priority based on its depth:</p>
<ul class="simple">
<li>if zero (default), no priority adjustment is made from depth</li>
<li><strong>a positive value will decrease the priority, i.e. higher depth
requests will be processed later</strong> ; this is commonly used when doing
breadth-first crawls (BFO)</li>
<li>a negative value will increase priority, i.e., higher depth requests
will be processed sooner (DFO)</li>
</ul>
<p>See also: <a class="reference internal" href="../faq.html#faq-bfo-dfo"><span class="std std-ref">Does Scrapy crawl in breadth-first or depth-first order?</span></a> about tuning Scrapy for BFO or DFO.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This setting adjusts priority <strong>in the opposite way</strong> compared to
other priority settings <a class="reference internal" href="#std:setting-REDIRECT_PRIORITY_ADJUST"><code class="xref std std-setting docutils literal"><span class="pre">REDIRECT_PRIORITY_ADJUST</span></code></a>
and <a class="reference internal" href="#std:setting-RETRY_PRIORITY_ADJUST"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_PRIORITY_ADJUST</span></code></a>.</p>
</div>
</div>
<div class="section" id="depth-stats">
<span id="std:setting-DEPTH_STATS"></span><h3>DEPTH_STATS<a class="headerlink" href="#depth-stats" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>Whether to collect maximum depth stats.</p>
</div>
<div class="section" id="depth-stats-verbose">
<span id="std:setting-DEPTH_STATS_VERBOSE"></span><h3>DEPTH_STATS_VERBOSE<a class="headerlink" href="#depth-stats-verbose" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>Whether to collect verbose depth stats. If this is enabled, the number of
requests for each depth is collected in the stats.</p>
</div>
<div class="section" id="dnscache-enabled">
<span id="std:setting-DNSCACHE_ENABLED"></span><h3>DNSCACHE_ENABLED<a class="headerlink" href="#dnscache-enabled" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether to enable DNS in-memory cache.</p>
</div>
<div class="section" id="dnscache-size">
<span id="std:setting-DNSCACHE_SIZE"></span><h3>DNSCACHE_SIZE<a class="headerlink" href="#dnscache-size" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">10000</span></code></p>
<p>DNS in-memory cache size.</p>
</div>
<div class="section" id="dns-timeout">
<span id="std:setting-DNS_TIMEOUT"></span><h3>DNS_TIMEOUT<a class="headerlink" href="#dns-timeout" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">60</span></code></p>
<p>Timeout for processing of DNS queries in seconds. Float is supported.</p>
</div>
<div class="section" id="downloader">
<span id="std:setting-DOWNLOADER"></span><h3>DOWNLOADER<a class="headerlink" href="#downloader" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.core.downloader.Downloader'</span></code></p>
<p>The downloader to use for crawling.</p>
</div>
<div class="section" id="downloader-httpclientfactory">
<span id="std:setting-DOWNLOADER_HTTPCLIENTFACTORY"></span><h3>DOWNLOADER_HTTPCLIENTFACTORY<a class="headerlink" href="#downloader-httpclientfactory" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'</span></code></p>
<p>Defines a Twisted <code class="docutils literal"><span class="pre">protocol.ClientFactory</span></code>  class to use for HTTP/1.0
connections (for <code class="docutils literal"><span class="pre">HTTP10DownloadHandler</span></code>).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">HTTP/1.0 is rarely used nowadays so you can safely ignore this setting,
unless you use Twisted&lt;11.1, or if you really want to use HTTP/1.0
and override <a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS_BASE</span></code></a> for <code class="docutils literal"><span class="pre">http(s)</span></code> scheme
accordingly, i.e. to
<code class="docutils literal"><span class="pre">'scrapy.core.downloader.handlers.http.HTTP10DownloadHandler'</span></code>.</p>
</div>
</div>
<div class="section" id="downloader-clientcontextfactory">
<span id="std:setting-DOWNLOADER_CLIENTCONTEXTFACTORY"></span><h3>DOWNLOADER_CLIENTCONTEXTFACTORY<a class="headerlink" href="#downloader-clientcontextfactory" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'</span></code></p>
<p>Represents the classpath to the ContextFactory to use.</p>
<p>Here, &#8220;ContextFactory&#8221; is a Twisted term for SSL/TLS contexts, defining
the TLS/SSL protocol version to use, whether to do certificate verification,
or even enable client-side authentication (and various other things).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Scrapy default context factory <strong>does NOT perform remote server
certificate verification</strong>. This is usually fine for web scraping.</p>
<p class="last">If you do need remote server certificate verification enabled,
Scrapy also has another context factory class that you can set,
<code class="docutils literal"><span class="pre">'scrapy.core.downloader.contextfactory.BrowserLikeContextFactory'</span></code>,
which uses the platform&#8217;s certificates to validate remote endpoints.
<strong>This is only available if you use Twisted&gt;=14.0.</strong></p>
</div>
<p>If you do use a custom ContextFactory, make sure it accepts a <code class="docutils literal"><span class="pre">method</span></code>
parameter at init (this is the <code class="docutils literal"><span class="pre">OpenSSL.SSL</span></code> method mapping
<a class="reference internal" href="#std:setting-DOWNLOADER_CLIENT_TLS_METHOD"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_CLIENT_TLS_METHOD</span></code></a>).</p>
</div>
<div class="section" id="downloader-client-tls-method">
<span id="std:setting-DOWNLOADER_CLIENT_TLS_METHOD"></span><h3>DOWNLOADER_CLIENT_TLS_METHOD<a class="headerlink" href="#downloader-client-tls-method" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'TLS'</span></code></p>
<p>Use this setting to customize the TLS/SSL method used by the default
HTTP/1.1 downloader.</p>
<p>This setting must be one of these string values:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">'TLS'</span></code>: maps to OpenSSL&#8217;s <code class="docutils literal"><span class="pre">TLS_method()</span></code> (a.k.a <code class="docutils literal"><span class="pre">SSLv23_method()</span></code>),
which allows protocol negotiation, starting from the highest supported
by the platform; <strong>default, recommended</strong></li>
<li><code class="docutils literal"><span class="pre">'TLSv1.0'</span></code>: this value forces HTTPS connections to use TLS version 1.0 ;
set this if you want the behavior of Scrapy&lt;1.1</li>
<li><code class="docutils literal"><span class="pre">'TLSv1.1'</span></code>: forces TLS version 1.1</li>
<li><code class="docutils literal"><span class="pre">'TLSv1.2'</span></code>: forces TLS version 1.2</li>
<li><code class="docutils literal"><span class="pre">'SSLv3'</span></code>: forces SSL version 3 (<strong>not recommended</strong>)</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">We recommend that you use PyOpenSSL&gt;=0.13 and Twisted&gt;=0.13
or above (Twisted&gt;=14.0 if you can).</p>
</div>
</div>
<div class="section" id="downloader-middlewares">
<span id="std:setting-DOWNLOADER_MIDDLEWARES"></span><h3>DOWNLOADER_MIDDLEWARES<a class="headerlink" href="#downloader-middlewares" title="Permalink to this headline">¶</a></h3>
<p>Default:: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the downloader middlewares enabled in your project, and their
orders. For more info see <a class="reference internal" href="downloader-middleware.html#topics-downloader-middleware-setting"><span class="std std-ref">Activating a downloader middleware</span></a>.</p>
</div>
<div class="section" id="downloader-middlewares-base">
<span id="std:setting-DOWNLOADER_MIDDLEWARES_BASE"></span><h3>DOWNLOADER_MIDDLEWARES_BASE<a class="headerlink" href="#downloader-middlewares-base" title="Permalink to this headline">¶</a></h3>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#39;</span><span class="p">:</span> <span class="mi">350</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#39;</span><span class="p">:</span> <span class="mi">400</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;</span><span class="p">:</span> <span class="mi">550</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware&#39;</span><span class="p">:</span> <span class="mi">560</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#39;</span><span class="p">:</span> <span class="mi">580</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;</span><span class="p">:</span> <span class="mi">590</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#39;</span><span class="p">:</span> <span class="mi">600</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#39;</span><span class="p">:</span> <span class="mi">700</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&#39;</span><span class="p">:</span> <span class="mi">750</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.stats.DownloaderStats&#39;</span><span class="p">:</span> <span class="mi">850</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&#39;</span><span class="p">:</span> <span class="mi">900</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the downloader middlewares enabled by default in Scrapy. Low
orders are closer to the engine, high orders are closer to the downloader. You
should never modify this setting in your project, modify
<a class="reference internal" href="#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> instead.  For more info see
<a class="reference internal" href="downloader-middleware.html#topics-downloader-middleware-setting"><span class="std std-ref">Activating a downloader middleware</span></a>.</p>
</div>
<div class="section" id="downloader-stats">
<span id="std:setting-DOWNLOADER_STATS"></span><h3>DOWNLOADER_STATS<a class="headerlink" href="#downloader-stats" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether to enable downloader stats collection.</p>
</div>
<div class="section" id="download-delay">
<span id="std:setting-DOWNLOAD_DELAY"></span><h3>DOWNLOAD_DELAY<a class="headerlink" href="#download-delay" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>The amount of time (in secs) that the downloader should wait before downloading
consecutive pages from the same website. This can be used to throttle the
crawling speed to avoid hitting servers too hard. Decimal numbers are
supported.  Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mf">0.25</span>    <span class="c1"># 250 ms of delay</span>
</pre></div>
</div>
<p>This setting is also affected by the <a class="reference internal" href="#std:setting-RANDOMIZE_DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></code></a>
setting (which is enabled by default). By default, Scrapy doesn&#8217;t wait a fixed
amount of time between requests, but uses a random interval between 0.5 * <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a> and 1.5 * <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a>.</p>
<p>When <a class="reference internal" href="#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> is non-zero, delays are enforced
per ip address instead of per domain.</p>
<p>You can also change this setting per spider by setting <code class="docutils literal"><span class="pre">download_delay</span></code>
spider attribute.</p>
</div>
<div class="section" id="download-handlers">
<span id="std:setting-DOWNLOAD_HANDLERS"></span><h3>DOWNLOAD_HANDLERS<a class="headerlink" href="#download-handlers" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the request downloader handlers enabled in your project.
See <a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS_BASE</span></code></a> for example format.</p>
</div>
<div class="section" id="download-handlers-base">
<span id="std:setting-DOWNLOAD_HANDLERS_BASE"></span><h3>DOWNLOAD_HANDLERS_BASE<a class="headerlink" href="#download-handlers-base" title="Permalink to this headline">¶</a></h3>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;file&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.file.FileDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;http&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;https&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;s3&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.s3.S3DownloadHandler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ftp&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.ftp.FTPDownloadHandler&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the request download handlers enabled by default in Scrapy.
You should never modify this setting in your project, modify
<a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></code></a> instead.</p>
<p>You can disable any of these download handlers by assigning <code class="docutils literal"><span class="pre">None</span></code> to their
URI scheme in <a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></code></a>. E.g., to disable the built-in FTP
handler (without replacement), place this in your <code class="docutils literal"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">DOWNLOAD_HANDLERS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;ftp&#39;</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="download-timeout">
<span id="std:setting-DOWNLOAD_TIMEOUT"></span><h3>DOWNLOAD_TIMEOUT<a class="headerlink" href="#download-timeout" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">180</span></code></p>
<p>The amount of time (in secs) that the downloader will wait before timing out.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This timeout can be set per spider using <code class="xref py py-attr docutils literal"><span class="pre">download_timeout</span></code>
spider attribute and per-request using <a class="reference internal" href="request-response.html#std:reqmeta-download_timeout"><code class="xref std std-reqmeta docutils literal"><span class="pre">download_timeout</span></code></a>
Request.meta key.</p>
</div>
</div>
<div class="section" id="download-maxsize">
<span id="std:setting-DOWNLOAD_MAXSIZE"></span><h3>DOWNLOAD_MAXSIZE<a class="headerlink" href="#download-maxsize" title="Permalink to this headline">¶</a></h3>
<p>Default: <cite>1073741824</cite> (1024MB)</p>
<p>The maximum response size (in bytes) that downloader will download.</p>
<p>If you want to disable it set to 0.</p>
<div class="admonition note" id="std:reqmeta-download_maxsize">
<p class="first admonition-title">Note</p>
<p>This size can be set per spider using <code class="xref py py-attr docutils literal"><span class="pre">download_maxsize</span></code>
spider attribute and per-request using <a class="reference internal" href="#std:reqmeta-download_maxsize"><code class="xref std std-reqmeta docutils literal"><span class="pre">download_maxsize</span></code></a>
Request.meta key.</p>
<p class="last">This feature needs Twisted &gt;= 11.1.</p>
</div>
</div>
<div class="section" id="download-warnsize">
<span id="std:setting-DOWNLOAD_WARNSIZE"></span><h3>DOWNLOAD_WARNSIZE<a class="headerlink" href="#download-warnsize" title="Permalink to this headline">¶</a></h3>
<p>Default: <cite>33554432</cite> (32MB)</p>
<p>The response size (in bytes) that downloader will start to warn.</p>
<p>If you want to disable it set to 0.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>This size can be set per spider using <code class="xref py py-attr docutils literal"><span class="pre">download_warnsize</span></code>
spider attribute and per-request using <code class="xref std std-reqmeta docutils literal"><span class="pre">download_warnsize</span></code>
Request.meta key.</p>
<p class="last">This feature needs Twisted &gt;= 11.1.</p>
</div>
</div>
<div class="section" id="dupefilter-class">
<span id="std:setting-DUPEFILTER_CLASS"></span><h3>DUPEFILTER_CLASS<a class="headerlink" href="#dupefilter-class" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.dupefilters.RFPDupeFilter'</span></code></p>
<p>The class used to detect and filter duplicate requests.</p>
<p>The default (<code class="docutils literal"><span class="pre">RFPDupeFilter</span></code>) filters based on request fingerprint using
the <code class="docutils literal"><span class="pre">scrapy.utils.request.request_fingerprint</span></code> function. In order to change
the way duplicates are checked you could subclass <code class="docutils literal"><span class="pre">RFPDupeFilter</span></code> and
override its <code class="docutils literal"><span class="pre">request_fingerprint</span></code> method. This method should accept
scrapy <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object and return its fingerprint
(a string).</p>
</div>
<div class="section" id="dupefilter-debug">
<span id="std:setting-DUPEFILTER_DEBUG"></span><h3>DUPEFILTER_DEBUG<a class="headerlink" href="#dupefilter-debug" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>By default, <code class="docutils literal"><span class="pre">RFPDupeFilter</span></code> only logs the first duplicate request.
Setting <a class="reference internal" href="#std:setting-DUPEFILTER_DEBUG"><code class="xref std std-setting docutils literal"><span class="pre">DUPEFILTER_DEBUG</span></code></a> to <code class="docutils literal"><span class="pre">True</span></code> will make it log all duplicate requests.</p>
</div>
<div class="section" id="editor">
<span id="std:setting-EDITOR"></span><h3>EDITOR<a class="headerlink" href="#editor" title="Permalink to this headline">¶</a></h3>
<p>Default: <cite>depends on the environment</cite></p>
<p>The editor to use for editing spiders with the <a class="reference internal" href="commands.html#std:command-edit"><code class="xref std std-command docutils literal"><span class="pre">edit</span></code></a> command. It
defaults to the <code class="docutils literal"><span class="pre">EDITOR</span></code> environment variable, if set. Otherwise, it defaults
to <code class="docutils literal"><span class="pre">vi</span></code> (on Unix systems) or the IDLE editor (on Windows).</p>
</div>
<div class="section" id="extensions">
<span id="std:setting-EXTENSIONS"></span><h3>EXTENSIONS<a class="headerlink" href="#extensions" title="Permalink to this headline">¶</a></h3>
<p>Default:: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the extensions enabled in your project, and their orders.</p>
</div>
<div class="section" id="extensions-base">
<span id="std:setting-EXTENSIONS_BASE"></span><h3>EXTENSIONS_BASE<a class="headerlink" href="#extensions-base" title="Permalink to this headline">¶</a></h3>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scrapy.extensions.corestats.CoreStats&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.telnet.TelnetConsole&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.memusage.MemoryUsage&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.memdebug.MemoryDebugger&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.closespider.CloseSpider&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.feedexport.FeedExporter&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.logstats.LogStats&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.spiderstate.SpiderState&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.throttle.AutoThrottle&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the extensions available by default in Scrapy, and their
orders. This setting contains all stable built-in extensions. Keep in mind that
some of them need to be enabled through a setting.</p>
<p>For more information See the <a class="reference internal" href="extensions.html#topics-extensions"><span class="std std-ref">extensions user guide</span></a>
and the <a class="reference internal" href="extensions.html#topics-extensions-ref"><span class="std std-ref">list of available extensions</span></a>.</p>
</div>
<div class="section" id="feed-tempdir">
<span id="std:setting-FEED_TEMPDIR"></span><h3>FEED_TEMPDIR<a class="headerlink" href="#feed-tempdir" title="Permalink to this headline">¶</a></h3>
<p>The Feed Temp dir allows you to set a custom folder to save crawler
temporary files before uploading with <a class="reference internal" href="feed-exports.html#topics-feed-storage-ftp"><span class="std std-ref">FTP feed storage</span></a> and
<a class="reference internal" href="feed-exports.html#topics-feed-storage-s3"><span class="std std-ref">Amazon S3</span></a>.</p>
</div>
<div class="section" id="item-pipelines">
<span id="std:setting-ITEM_PIPELINES"></span><h3>ITEM_PIPELINES<a class="headerlink" href="#item-pipelines" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the item pipelines to use, and their orders. Order values are
arbitrary, but it is customary to define them in the 0-1000 range. Lower orders
process before higher orders.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;mybot.pipelines.validate.ValidateMyItem&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s1">&#39;mybot.pipelines.validate.StoreMyItem&#39;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="item-pipelines-base">
<span id="std:setting-ITEM_PIPELINES_BASE"></span><h3>ITEM_PIPELINES_BASE<a class="headerlink" href="#item-pipelines-base" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the pipelines enabled by default in Scrapy. You should never
modify this setting in your project, modify <a class="reference internal" href="#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></code></a> instead.</p>
</div>
<div class="section" id="log-enabled">
<span id="std:setting-LOG_ENABLED"></span><h3>LOG_ENABLED<a class="headerlink" href="#log-enabled" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether to enable logging.</p>
</div>
<div class="section" id="log-encoding">
<span id="std:setting-LOG_ENCODING"></span><h3>LOG_ENCODING<a class="headerlink" href="#log-encoding" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'utf-8'</span></code></p>
<p>The encoding to use for logging.</p>
</div>
<div class="section" id="log-file">
<span id="std:setting-LOG_FILE"></span><h3>LOG_FILE<a class="headerlink" href="#log-file" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>File name to use for logging output. If <code class="docutils literal"><span class="pre">None</span></code>, standard error will be used.</p>
</div>
<div class="section" id="log-format">
<span id="std:setting-LOG_FORMAT"></span><h3>LOG_FORMAT<a class="headerlink" href="#log-format" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'%(asctime)s</span> <span class="pre">[%(name)s]</span> <span class="pre">%(levelname)s:</span> <span class="pre">%(message)s'</span></code></p>
<p>String for formatting log messsages. Refer to the <a class="reference external" href="https://docs.python.org/2/library/logging.html#logrecord-attributes">Python logging documentation</a> for the whole list of available
placeholders.</p>
</div>
<div class="section" id="log-dateformat">
<span id="std:setting-LOG_DATEFORMAT"></span><h3>LOG_DATEFORMAT<a class="headerlink" href="#log-dateformat" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'%Y-%m-%d</span> <span class="pre">%H:%M:%S'</span></code></p>
<p>String for formatting date/time, expansion of the <code class="docutils literal"><span class="pre">%(asctime)s</span></code> placeholder
in <a class="reference internal" href="#std:setting-LOG_FORMAT"><code class="xref std std-setting docutils literal"><span class="pre">LOG_FORMAT</span></code></a>. Refer to the <a class="reference external" href="https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior">Python datetime documentation</a> for the whole list of available
directives.</p>
</div>
<div class="section" id="log-level">
<span id="std:setting-LOG_LEVEL"></span><h3>LOG_LEVEL<a class="headerlink" href="#log-level" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'DEBUG'</span></code></p>
<p>Minimum level to log. Available levels are: CRITICAL, ERROR, WARNING,
INFO, DEBUG. For more info see <a class="reference internal" href="logging.html#topics-logging"><span class="std std-ref">Logging</span></a>.</p>
</div>
<div class="section" id="log-stdout">
<span id="std:setting-LOG_STDOUT"></span><h3>LOG_STDOUT<a class="headerlink" href="#log-stdout" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If <code class="docutils literal"><span class="pre">True</span></code>, all standard output (and error) of your process will be redirected
to the log. For example if you <code class="docutils literal"><span class="pre">print</span> <span class="pre">'hello'</span></code> it will appear in the Scrapy
log.</p>
</div>
<div class="section" id="log-short-names">
<span id="std:setting-LOG_SHORT_NAMES"></span><h3>LOG_SHORT_NAMES<a class="headerlink" href="#log-short-names" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If <code class="docutils literal"><span class="pre">True</span></code>, the logs will just contain the root path. If it is set to <code class="docutils literal"><span class="pre">False</span></code>
then it displays the component responsible for the log output</p>
</div>
<div class="section" id="memdebug-enabled">
<span id="std:setting-MEMDEBUG_ENABLED"></span><h3>MEMDEBUG_ENABLED<a class="headerlink" href="#memdebug-enabled" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Whether to enable memory debugging.</p>
</div>
<div class="section" id="memdebug-notify">
<span id="std:setting-MEMDEBUG_NOTIFY"></span><h3>MEMDEBUG_NOTIFY<a class="headerlink" href="#memdebug-notify" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code></p>
<p>When memory debugging is enabled a memory report will be sent to the specified
addresses if this setting is not empty, otherwise the report will be written to
the log.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">MEMDEBUG_NOTIFY</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;user@example.com&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="memusage-enabled">
<span id="std:setting-MEMUSAGE_ENABLED"></span><h3>MEMUSAGE_ENABLED<a class="headerlink" href="#memusage-enabled" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>Whether to enable the memory usage extension that will shutdown the Scrapy
process when it exceeds a memory limit, and also notify by email when that
happened.</p>
<p>See <a class="reference internal" href="extensions.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
</div>
<div class="section" id="memusage-limit-mb">
<span id="std:setting-MEMUSAGE_LIMIT_MB"></span><h3>MEMUSAGE_LIMIT_MB<a class="headerlink" href="#memusage-limit-mb" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The maximum amount of memory to allow (in megabytes) before shutting down
Scrapy  (if MEMUSAGE_ENABLED is True). If zero, no check will be performed.</p>
<p>See <a class="reference internal" href="extensions.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
</div>
<div class="section" id="memusage-check-interval-seconds">
<span id="std:setting-MEMUSAGE_CHECK_INTERVAL_SECONDS"></span><h3>MEMUSAGE_CHECK_INTERVAL_SECONDS<a class="headerlink" href="#memusage-check-interval-seconds" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.1.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">60.0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The <a class="reference internal" href="extensions.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>
checks the current memory usage, versus the limits set by
<a class="reference internal" href="#std:setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a> and <a class="reference internal" href="#std:setting-MEMUSAGE_WARNING_MB"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_WARNING_MB</span></code></a>,
at fixed time intervals.</p>
<p>This sets the length of these intervals, in seconds.</p>
<p>See <a class="reference internal" href="extensions.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
</div>
<div class="section" id="memusage-notify-mail">
<span id="std:setting-MEMUSAGE_NOTIFY_MAIL"></span><h3>MEMUSAGE_NOTIFY_MAIL<a class="headerlink" href="#memusage-notify-mail" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>A list of emails to notify if the memory limit has been reached.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">MEMUSAGE_NOTIFY_MAIL</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;user@example.com&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="extensions.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
</div>
<div class="section" id="memusage-report">
<span id="std:setting-MEMUSAGE_REPORT"></span><h3>MEMUSAGE_REPORT<a class="headerlink" href="#memusage-report" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>Whether to send a memory usage report after each spider has been closed.</p>
<p>See <a class="reference internal" href="extensions.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
</div>
<div class="section" id="memusage-warning-mb">
<span id="std:setting-MEMUSAGE_WARNING_MB"></span><h3>MEMUSAGE_WARNING_MB<a class="headerlink" href="#memusage-warning-mb" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The maximum amount of memory to allow (in megabytes) before sending a warning
email notifying about it. If zero, no warning will be produced.</p>
</div>
<div class="section" id="newspider-module">
<span id="std:setting-NEWSPIDER_MODULE"></span><h3>NEWSPIDER_MODULE<a class="headerlink" href="#newspider-module" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">''</span></code></p>
<p>Module where to create new spiders using the <a class="reference internal" href="commands.html#std:command-genspider"><code class="xref std std-command docutils literal"><span class="pre">genspider</span></code></a> command.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">NEWSPIDER_MODULE</span> <span class="o">=</span> <span class="s1">&#39;mybot.spiders_dev&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="randomize-download-delay">
<span id="std:setting-RANDOMIZE_DOWNLOAD_DELAY"></span><h3>RANDOMIZE_DOWNLOAD_DELAY<a class="headerlink" href="#randomize-download-delay" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>If enabled, Scrapy will wait a random amount of time (between 0.5 * <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a> and 1.5 * <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a>) while fetching requests from the same
website.</p>
<p>This randomization decreases the chance of the crawler being detected (and
subsequently blocked) by sites which analyze requests looking for statistically
significant similarities in the time between their requests.</p>
<p>The randomization policy is the same used by <a class="reference external" href="http://www.gnu.org/software/wget/manual/wget.html">wget</a> <code class="docutils literal"><span class="pre">--random-wait</span></code> option.</p>
<p>If <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a> is zero (default) this option has no effect.</p>
</div>
<div class="section" id="reactor-threadpool-maxsize">
<span id="std:setting-REACTOR_THREADPOOL_MAXSIZE"></span><h3>REACTOR_THREADPOOL_MAXSIZE<a class="headerlink" href="#reactor-threadpool-maxsize" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">10</span></code></p>
<p>The maximum limit for Twisted Reactor thread pool size. This is common
multi-purpose thread pool used by various Scrapy components. Threaded
DNS Resolver, BlockingFeedStorage, S3FilesStore just to name a few. Increase
this value if you&#8217;re experiencing problems with insufficient blocking IO.</p>
</div>
<div class="section" id="redirect-max-times">
<span id="std:setting-REDIRECT_MAX_TIMES"></span><h3>REDIRECT_MAX_TIMES<a class="headerlink" href="#redirect-max-times" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">20</span></code></p>
<p>Defines the maximum times a request can be redirected. After this maximum the
request&#8217;s response is returned as is. We used Firefox default value for the
same task.</p>
</div>
<div class="section" id="redirect-priority-adjust">
<span id="std:setting-REDIRECT_PRIORITY_ADJUST"></span><h3>REDIRECT_PRIORITY_ADJUST<a class="headerlink" href="#redirect-priority-adjust" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">+2</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.downloadermiddlewares.redirect.RedirectMiddleware</span></code></p>
<p>Adjust redirect request priority relative to original request:</p>
<ul class="simple">
<li><strong>a positive priority adjust (default) means higher priority.</strong></li>
<li>a negative priority adjust means lower priority.</li>
</ul>
</div>
<div class="section" id="retry-priority-adjust">
<span id="std:setting-RETRY_PRIORITY_ADJUST"></span><h3>RETRY_PRIORITY_ADJUST<a class="headerlink" href="#retry-priority-adjust" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">-1</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.downloadermiddlewares.retry.RetryMiddleware</span></code></p>
<p>Adjust retry request priority relative to original request:</p>
<ul class="simple">
<li>a positive priority adjust means higher priority.</li>
<li><strong>a negative priority adjust (default) means lower priority.</strong></li>
</ul>
</div>
<div class="section" id="robotstxt-obey">
<span id="std:setting-ROBOTSTXT_OBEY"></span><h3>ROBOTSTXT_OBEY<a class="headerlink" href="#robotstxt-obey" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.downloadermiddlewares.robotstxt</span></code></p>
<p>If enabled, Scrapy will respect robots.txt policies. For more information see
<a class="reference internal" href="downloader-middleware.html#topics-dlmw-robots"><span class="std std-ref">RobotsTxtMiddleware</span></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">While the default value is <code class="docutils literal"><span class="pre">False</span></code> for historical reasons,
this option is enabled by default in settings.py file generated
by <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">startproject</span></code> command.</p>
</div>
</div>
<div class="section" id="scheduler">
<span id="std:setting-SCHEDULER"></span><h3>SCHEDULER<a class="headerlink" href="#scheduler" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.core.scheduler.Scheduler'</span></code></p>
<p>The scheduler to use for crawling.</p>
</div>
<div class="section" id="scheduler-debug">
<span id="std:setting-SCHEDULER_DEBUG"></span><h3>SCHEDULER_DEBUG<a class="headerlink" href="#scheduler-debug" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Setting to <code class="docutils literal"><span class="pre">True</span></code> will log debug information about the requests scheduler.
This currently logs (only once) if the requests cannot be serialized to disk.
Stats counter (<code class="docutils literal"><span class="pre">scheduler/unserializable</span></code>) tracks the number of times this happens.</p>
<p>Example entry in logs:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="mi">1956</span><span class="o">-</span><span class="mo">01</span><span class="o">-</span><span class="mi">31</span> <span class="mo">00</span><span class="p">:</span><span class="mo">00</span><span class="p">:</span><span class="mo">00</span><span class="o">+</span><span class="mi">0800</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">scheduler</span><span class="p">]</span> <span class="n">ERROR</span><span class="p">:</span> <span class="n">Unable</span> <span class="n">to</span> <span class="n">serialize</span> <span class="n">request</span><span class="p">:</span>
<span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">&gt;</span> <span class="o">-</span> <span class="n">reason</span><span class="p">:</span> <span class="n">cannot</span> <span class="n">serialize</span> <span class="o">&lt;</span><span class="n">Request</span> <span class="n">at</span> <span class="mh">0x9a7c7ec</span><span class="o">&gt;</span>
<span class="p">(</span><span class="nb">type</span> <span class="n">Request</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">-</span> <span class="n">no</span> <span class="n">more</span> <span class="n">unserializable</span> <span class="n">requests</span> <span class="n">will</span> <span class="n">be</span> <span class="n">logged</span>
<span class="p">(</span><span class="n">see</span> <span class="s1">&#39;scheduler/unserializable&#39;</span> <span class="n">stats</span> <span class="n">counter</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="scheduler-disk-queue">
<span id="std:setting-SCHEDULER_DISK_QUEUE"></span><h3>SCHEDULER_DISK_QUEUE<a class="headerlink" href="#scheduler-disk-queue" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.squeues.PickleLifoDiskQueue'</span></code></p>
<p>Type of disk queue that will be used by scheduler. Other available types are
<code class="docutils literal"><span class="pre">scrapy.squeues.PickleFifoDiskQueue</span></code>, <code class="docutils literal"><span class="pre">scrapy.squeues.MarshalFifoDiskQueue</span></code>,
<code class="docutils literal"><span class="pre">scrapy.squeues.MarshalLifoDiskQueue</span></code>.</p>
</div>
<div class="section" id="scheduler-memory-queue">
<span id="std:setting-SCHEDULER_MEMORY_QUEUE"></span><h3>SCHEDULER_MEMORY_QUEUE<a class="headerlink" href="#scheduler-memory-queue" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.squeues.LifoMemoryQueue'</span></code></p>
<p>Type of in-memory queue used by scheduler. Other available type is:
<code class="docutils literal"><span class="pre">scrapy.squeues.FifoMemoryQueue</span></code>.</p>
</div>
<div class="section" id="scheduler-priority-queue">
<span id="std:setting-SCHEDULER_PRIORITY_QUEUE"></span><h3>SCHEDULER_PRIORITY_QUEUE<a class="headerlink" href="#scheduler-priority-queue" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'queuelib.PriorityQueue'</span></code></p>
<p>Type of priority queue used by scheduler.</p>
</div>
<div class="section" id="spider-contracts">
<span id="std:setting-SPIDER_CONTRACTS"></span><h3>SPIDER_CONTRACTS<a class="headerlink" href="#spider-contracts" title="Permalink to this headline">¶</a></h3>
<p>Default:: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the spider contracts enabled in your project, used for
testing spiders. For more info see <a class="reference internal" href="contracts.html#topics-contracts"><span class="std std-ref">Spiders Contracts</span></a>.</p>
</div>
<div class="section" id="spider-contracts-base">
<span id="std:setting-SPIDER_CONTRACTS_BASE"></span><h3>SPIDER_CONTRACTS_BASE<a class="headerlink" href="#spider-contracts-base" title="Permalink to this headline">¶</a></h3>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scrapy.contracts.default.UrlContract&#39;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.contracts.default.ReturnsContract&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.contracts.default.ScrapesContract&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the scrapy contracts enabled by default in Scrapy. You should
never modify this setting in your project, modify <a class="reference internal" href="#std:setting-SPIDER_CONTRACTS"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_CONTRACTS</span></code></a>
instead. For more info see <a class="reference internal" href="contracts.html#topics-contracts"><span class="std std-ref">Spiders Contracts</span></a>.</p>
<p>You can disable any of these contracts by assigning <code class="docutils literal"><span class="pre">None</span></code> to their class
path in <a class="reference internal" href="#std:setting-SPIDER_CONTRACTS"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_CONTRACTS</span></code></a>. E.g., to disable the built-in
<code class="docutils literal"><span class="pre">ScrapesContract</span></code>, place this in your <code class="docutils literal"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">SPIDER_CONTRACTS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;scrapy.contracts.default.ScrapesContract&#39;</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="spider-loader-class">
<span id="std:setting-SPIDER_LOADER_CLASS"></span><h3>SPIDER_LOADER_CLASS<a class="headerlink" href="#spider-loader-class" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.spiderloader.SpiderLoader'</span></code></p>
<p>The class that will be used for loading spiders, which must implement the
<a class="reference internal" href="api.html#topics-api-spiderloader"><span class="std std-ref">SpiderLoader API</span></a>.</p>
</div>
<div class="section" id="spider-loader-warn-only">
<span id="std:setting-SPIDER_LOADER_WARN_ONLY"></span><h3>SPIDER_LOADER_WARN_ONLY<a class="headerlink" href="#spider-loader-warn-only" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.3.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>By default, when scrapy tries to import spider classes from <a class="reference internal" href="#std:setting-SPIDER_MODULES"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MODULES</span></code></a>,
it will fail loudly if there is any <code class="docutils literal"><span class="pre">ImportError</span></code> exception.
But you can choose to silence this exception and turn it into a simple
warning by setting <code class="docutils literal"><span class="pre">SPIDER_LOADER_WARN_ONLY</span> <span class="pre">=</span> <span class="pre">True</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Some <a class="reference internal" href="commands.html#topics-commands"><span class="std std-ref">scrapy commands</span></a> run with this setting to <code class="docutils literal"><span class="pre">True</span></code>
already (i.e. they will only issue a warning and will not fail)
since they do not actually need to load spider classes to work:
<a class="reference internal" href="commands.html#std:command-runspider"><code class="xref std std-command docutils literal"><span class="pre">scrapy</span> <span class="pre">runspider</span></code></a>,
<a class="reference internal" href="commands.html#std:command-settings"><code class="xref std std-command docutils literal"><span class="pre">scrapy</span> <span class="pre">settings</span></code></a>,
<a class="reference internal" href="commands.html#std:command-startproject"><code class="xref std std-command docutils literal"><span class="pre">scrapy</span> <span class="pre">startproject</span></code></a>,
<a class="reference internal" href="commands.html#std:command-version"><code class="xref std std-command docutils literal"><span class="pre">scrapy</span> <span class="pre">version</span></code></a>.</p>
</div>
</div>
<div class="section" id="spider-middlewares">
<span id="std:setting-SPIDER_MIDDLEWARES"></span><h3>SPIDER_MIDDLEWARES<a class="headerlink" href="#spider-middlewares" title="Permalink to this headline">¶</a></h3>
<p>Default:: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the spider middlewares enabled in your project, and their
orders. For more info see <a class="reference internal" href="spider-middleware.html#topics-spider-middleware-setting"><span class="std std-ref">Activating a spider middleware</span></a>.</p>
</div>
<div class="section" id="spider-middlewares-base">
<span id="std:setting-SPIDER_MIDDLEWARES_BASE"></span><h3>SPIDER_MIDDLEWARES_BASE<a class="headerlink" href="#spider-middlewares-base" title="Permalink to this headline">¶</a></h3>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.referer.RefererMiddleware&#39;</span><span class="p">:</span> <span class="mi">700</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&#39;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.depth.DepthMiddleware&#39;</span><span class="p">:</span> <span class="mi">900</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the spider middlewares enabled by default in Scrapy, and
their orders. Low orders are closer to the engine, high orders are closer to
the spider. For more info see <a class="reference internal" href="spider-middleware.html#topics-spider-middleware-setting"><span class="std std-ref">Activating a spider middleware</span></a>.</p>
</div>
<div class="section" id="spider-modules">
<span id="std:setting-SPIDER_MODULES"></span><h3>SPIDER_MODULES<a class="headerlink" href="#spider-modules" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code></p>
<p>A list of modules where Scrapy will look for spiders.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">SPIDER_MODULES</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mybot.spiders_prod&#39;</span><span class="p">,</span> <span class="s1">&#39;mybot.spiders_dev&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="stats-class">
<span id="std:setting-STATS_CLASS"></span><h3>STATS_CLASS<a class="headerlink" href="#stats-class" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.statscollectors.MemoryStatsCollector'</span></code></p>
<p>The class to use for collecting stats, who must implement the
<a class="reference internal" href="api.html#topics-api-stats"><span class="std std-ref">Stats Collector API</span></a>.</p>
</div>
<div class="section" id="stats-dump">
<span id="std:setting-STATS_DUMP"></span><h3>STATS_DUMP<a class="headerlink" href="#stats-dump" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Dump the <a class="reference internal" href="stats.html#topics-stats"><span class="std std-ref">Scrapy stats</span></a> (to the Scrapy log) once the spider
finishes.</p>
<p>For more info see: <a class="reference internal" href="stats.html#topics-stats"><span class="std std-ref">Stats Collection</span></a>.</p>
</div>
<div class="section" id="statsmailer-rcpts">
<span id="std:setting-STATSMAILER_RCPTS"></span><h3>STATSMAILER_RCPTS<a class="headerlink" href="#statsmailer-rcpts" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code> (empty list)</p>
<p>Send Scrapy stats after spiders finish scraping. See
<code class="xref py py-class docutils literal"><span class="pre">StatsMailer</span></code> for more info.</p>
</div>
<div class="section" id="telnetconsole-enabled">
<span id="std:setting-TELNETCONSOLE_ENABLED"></span><h3>TELNETCONSOLE_ENABLED<a class="headerlink" href="#telnetconsole-enabled" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>A boolean which specifies if the <a class="reference internal" href="telnetconsole.html#topics-telnetconsole"><span class="std std-ref">telnet console</span></a>
will be enabled (provided its extension is also enabled).</p>
</div>
<div class="section" id="telnetconsole-port">
<span id="std:setting-TELNETCONSOLE_PORT"></span><h3>TELNETCONSOLE_PORT<a class="headerlink" href="#telnetconsole-port" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">[6023,</span> <span class="pre">6073]</span></code></p>
<p>The port range to use for the telnet console. If set to <code class="docutils literal"><span class="pre">None</span></code> or <code class="docutils literal"><span class="pre">0</span></code>, a
dynamically assigned port is used. For more info see
<a class="reference internal" href="telnetconsole.html#topics-telnetconsole"><span class="std std-ref">Telnet Console</span></a>.</p>
</div>
<div class="section" id="templates-dir">
<span id="std:setting-TEMPLATES_DIR"></span><h3>TEMPLATES_DIR<a class="headerlink" href="#templates-dir" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">templates</span></code> dir inside scrapy module</p>
<p>The directory where to look for templates when creating new projects with
<a class="reference internal" href="commands.html#std:command-startproject"><code class="xref std std-command docutils literal"><span class="pre">startproject</span></code></a> command and new spiders with <a class="reference internal" href="commands.html#std:command-genspider"><code class="xref std std-command docutils literal"><span class="pre">genspider</span></code></a>
command.</p>
<p>The project name must not conflict with the name of custom files or directories
in the <code class="docutils literal"><span class="pre">project</span></code> subdirectory.</p>
</div>
<div class="section" id="urllength-limit">
<span id="std:setting-URLLENGTH_LIMIT"></span><h3>URLLENGTH_LIMIT<a class="headerlink" href="#urllength-limit" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">2083</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">spidermiddlewares.urllength</span></code></p>
<p>The maximum URL length to allow for crawled URLs. For more information about
the default value for this setting see: <a class="reference external" href="http://www.boutell.com/newfaq/misc/urllength.html">http://www.boutell.com/newfaq/misc/urllength.html</a></p>
</div>
<div class="section" id="user-agent">
<span id="std:setting-USER_AGENT"></span><h3>USER_AGENT<a class="headerlink" href="#user-agent" title="Permalink to this headline">¶</a></h3>
<p>Default: <code class="docutils literal"><span class="pre">&quot;Scrapy/VERSION</span> <span class="pre">(+http://scrapy.org)&quot;</span></code></p>
<p>The default User-Agent to use when crawling, unless overridden.</p>
</div>
<div class="section" id="settings-documented-elsewhere">
<h3>Settings documented elsewhere:<a class="headerlink" href="#settings-documented-elsewhere" title="Permalink to this headline">¶</a></h3>
<p>The following settings are documented elsewhere, please check each specific
case to see how to enable and use them.</p>
<ul class="simple">
<li><a class="reference internal" href="downloader-middleware.html#std:setting-AJAXCRAWL_ENABLED">AJAXCRAWL_ENABLED</a></li>
<li><a class="reference internal" href="autothrottle.html#std:setting-AUTOTHROTTLE_DEBUG">AUTOTHROTTLE_DEBUG</a></li>
<li><a class="reference internal" href="autothrottle.html#std:setting-AUTOTHROTTLE_ENABLED">AUTOTHROTTLE_ENABLED</a></li>
<li><a class="reference internal" href="autothrottle.html#std:setting-AUTOTHROTTLE_MAX_DELAY">AUTOTHROTTLE_MAX_DELAY</a></li>
<li><a class="reference internal" href="autothrottle.html#std:setting-AUTOTHROTTLE_START_DELAY">AUTOTHROTTLE_START_DELAY</a></li>
<li><a class="reference internal" href="autothrottle.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY">AUTOTHROTTLE_TARGET_CONCURRENCY</a></li>
<li><a class="reference internal" href="extensions.html#std:setting-CLOSESPIDER_ERRORCOUNT">CLOSESPIDER_ERRORCOUNT</a></li>
<li><a class="reference internal" href="extensions.html#std:setting-CLOSESPIDER_ITEMCOUNT">CLOSESPIDER_ITEMCOUNT</a></li>
<li><a class="reference internal" href="extensions.html#std:setting-CLOSESPIDER_PAGECOUNT">CLOSESPIDER_PAGECOUNT</a></li>
<li><a class="reference internal" href="extensions.html#std:setting-CLOSESPIDER_TIMEOUT">CLOSESPIDER_TIMEOUT</a></li>
<li><a class="reference internal" href="commands.html#std:setting-COMMANDS_MODULE">COMMANDS_MODULE</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-COMPRESSION_ENABLED">COMPRESSION_ENABLED</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-COOKIES_DEBUG">COOKIES_DEBUG</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-COOKIES_ENABLED">COOKIES_ENABLED</a></li>
<li><a class="reference internal" href="feed-exports.html#std:setting-FEED_EXPORTERS">FEED_EXPORTERS</a></li>
<li><a class="reference internal" href="feed-exports.html#std:setting-FEED_EXPORTERS_BASE">FEED_EXPORTERS_BASE</a></li>
<li><a class="reference internal" href="feed-exports.html#std:setting-FEED_EXPORT_ENCODING">FEED_EXPORT_ENCODING</a></li>
<li><a class="reference internal" href="feed-exports.html#std:setting-FEED_EXPORT_FIELDS">FEED_EXPORT_FIELDS</a></li>
<li><a class="reference internal" href="feed-exports.html#std:setting-FEED_FORMAT">FEED_FORMAT</a></li>
<li><a class="reference internal" href="feed-exports.html#std:setting-FEED_STORAGES">FEED_STORAGES</a></li>
<li><a class="reference internal" href="feed-exports.html#std:setting-FEED_STORAGES_BASE">FEED_STORAGES_BASE</a></li>
<li><a class="reference internal" href="feed-exports.html#std:setting-FEED_STORE_EMPTY">FEED_STORE_EMPTY</a></li>
<li><a class="reference internal" href="feed-exports.html#std:setting-FEED_URI">FEED_URI</a></li>
<li><a class="reference internal" href="media-pipeline.html#std:setting-FILES_EXPIRES">FILES_EXPIRES</a></li>
<li><a class="reference internal" href="media-pipeline.html#std:setting-FILES_RESULT_FIELD">FILES_RESULT_FIELD</a></li>
<li><a class="reference internal" href="media-pipeline.html#std:setting-FILES_STORE">FILES_STORE</a></li>
<li><a class="reference internal" href="media-pipeline.html#std:setting-FILES_STORE_S3_ACL">FILES_STORE_S3_ACL</a></li>
<li><a class="reference internal" href="media-pipeline.html#std:setting-FILES_URLS_FIELD">FILES_URLS_FIELD</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-HTTPCACHE_ALWAYS_STORE">HTTPCACHE_ALWAYS_STORE</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-HTTPCACHE_DBM_MODULE">HTTPCACHE_DBM_MODULE</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-HTTPCACHE_DIR">HTTPCACHE_DIR</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-HTTPCACHE_ENABLED">HTTPCACHE_ENABLED</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-HTTPCACHE_EXPIRATION_SECS">HTTPCACHE_EXPIRATION_SECS</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-HTTPCACHE_GZIP">HTTPCACHE_GZIP</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-HTTPCACHE_IGNORE_HTTP_CODES">HTTPCACHE_IGNORE_HTTP_CODES</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-HTTPCACHE_IGNORE_MISSING">HTTPCACHE_IGNORE_MISSING</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS">HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-HTTPCACHE_IGNORE_SCHEMES">HTTPCACHE_IGNORE_SCHEMES</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-HTTPCACHE_POLICY">HTTPCACHE_POLICY</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-HTTPCACHE_STORAGE">HTTPCACHE_STORAGE</a></li>
<li><a class="reference internal" href="spider-middleware.html#std:setting-HTTPERROR_ALLOWED_CODES">HTTPERROR_ALLOWED_CODES</a></li>
<li><a class="reference internal" href="spider-middleware.html#std:setting-HTTPERROR_ALLOW_ALL">HTTPERROR_ALLOW_ALL</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-HTTPPROXY_AUTH_ENCODING">HTTPPROXY_AUTH_ENCODING</a></li>
<li><a class="reference internal" href="media-pipeline.html#std:setting-IMAGES_EXPIRES">IMAGES_EXPIRES</a></li>
<li><a class="reference internal" href="media-pipeline.html#std:setting-IMAGES_MIN_HEIGHT">IMAGES_MIN_HEIGHT</a></li>
<li><a class="reference internal" href="media-pipeline.html#std:setting-IMAGES_MIN_WIDTH">IMAGES_MIN_WIDTH</a></li>
<li><a class="reference internal" href="media-pipeline.html#std:setting-IMAGES_RESULT_FIELD">IMAGES_RESULT_FIELD</a></li>
<li><a class="reference internal" href="media-pipeline.html#std:setting-IMAGES_STORE">IMAGES_STORE</a></li>
<li><a class="reference internal" href="media-pipeline.html#std:setting-IMAGES_STORE_S3_ACL">IMAGES_STORE_S3_ACL</a></li>
<li><a class="reference internal" href="media-pipeline.html#std:setting-IMAGES_THUMBS">IMAGES_THUMBS</a></li>
<li><a class="reference internal" href="media-pipeline.html#std:setting-IMAGES_URLS_FIELD">IMAGES_URLS_FIELD</a></li>
<li><a class="reference internal" href="email.html#std:setting-MAIL_FROM">MAIL_FROM</a></li>
<li><a class="reference internal" href="email.html#std:setting-MAIL_HOST">MAIL_HOST</a></li>
<li><a class="reference internal" href="email.html#std:setting-MAIL_PASS">MAIL_PASS</a></li>
<li><a class="reference internal" href="email.html#std:setting-MAIL_PORT">MAIL_PORT</a></li>
<li><a class="reference internal" href="email.html#std:setting-MAIL_SSL">MAIL_SSL</a></li>
<li><a class="reference internal" href="email.html#std:setting-MAIL_TLS">MAIL_TLS</a></li>
<li><a class="reference internal" href="email.html#std:setting-MAIL_USER">MAIL_USER</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-METAREFRESH_ENABLED">METAREFRESH_ENABLED</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-METAREFRESH_MAXDELAY">METAREFRESH_MAXDELAY</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-REDIRECT_ENABLED">REDIRECT_ENABLED</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-REDIRECT_MAX_TIMES">REDIRECT_MAX_TIMES</a></li>
<li><a class="reference internal" href="spider-middleware.html#std:setting-REFERER_ENABLED">REFERER_ENABLED</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-RETRY_ENABLED">RETRY_ENABLED</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-RETRY_HTTP_CODES">RETRY_HTTP_CODES</a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:setting-RETRY_TIMES">RETRY_TIMES</a></li>
<li><a class="reference internal" href="telnetconsole.html#std:setting-TELNETCONSOLE_HOST">TELNETCONSOLE_HOST</a></li>
<li><a class="reference internal" href="telnetconsole.html#std:setting-TELNETCONSOLE_PORT">TELNETCONSOLE_PORT</a></li>
</ul>
</div>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="exceptions.html" class="btn btn-neutral float-right" title="Exceptions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="link-extractors.html" class="btn btn-neutral" title="Link Extractors" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>f46b5f2b</code>.
        </span>
      

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: latest
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Versions</dt>
        
          <dd><a href="/en/latest/">latest</a></dd>
        
          <dd><a href="/en/stable/">stable</a></dd>
        
          <dd><a href="/en/1.3/">1.3</a></dd>
        
          <dd><a href="/en/1.2/">1.2</a></dd>
        
          <dd><a href="/en/1.1/">1.1</a></dd>
        
          <dd><a href="/en/1.0/">1.0</a></dd>
        
          <dd><a href="/en/0.24/">0.24</a></dd>
        
          <dd><a href="/en/0.22/">0.22</a></dd>
        
          <dd><a href="/en/0.20/">0.20</a></dd>
        
          <dd><a href="/en/0.18/">0.18</a></dd>
        
          <dd><a href="/en/0.16/">0.16</a></dd>
        
          <dd><a href="/en/0.14/">0.14</a></dd>
        
          <dd><a href="/en/0.12/">0.12</a></dd>
        
          <dd><a href="/en/0.10.3/">0.10.3</a></dd>
        
          <dd><a href="/en/0.9/">0.9</a></dd>
        
          <dd><a href="/en/master/">master</a></dd>
        
      </dl>
      <dl>
        <dt>Downloads</dt>
        
          <dd><a href="//readthedocs.org/projects/scrapy/downloads/pdf/latest/">pdf</a></dd>
        
          <dd><a href="//readthedocs.org/projects/scrapy/downloads/htmlzip/latest/">htmlzip</a></dd>
        
          <dd><a href="//readthedocs.org/projects/scrapy/downloads/epub/latest/">epub</a></dd>
        
      </dl>
      <dl>
        <dt>On Read the Docs</dt>
          <dd>
            <a href="//readthedocs.org/projects/scrapy/?fromdocs=scrapy">Project Home</a>
          </dd>
          <dd>
            <a href="//readthedocs.org/builds/scrapy/?fromdocs=scrapy">Builds</a>
          </dd>
      </dl>
      <hr/>
      Free document hosting provided by <a href="http://www.readthedocs.org">Read the Docs</a>.

    </div>
  </div>



  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'1.3.3',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="https://media.readthedocs.org/javascript/jquery/jquery-2.0.3.min.js"></script>
      <script type="text/javascript" src="https://media.readthedocs.org/javascript/jquery/jquery-migrate-1.2.1.min.js"></script>
      <script type="text/javascript" src="https://media.readthedocs.org/javascript/underscore.js"></script>
      <script type="text/javascript" src="https://media.readthedocs.org/javascript/doctools.js"></script>
      <script type="text/javascript" src="https://media.readthedocs.org/javascript/readthedocs-doc-embed.js"></script>

  

  
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  
 
<script type="text/javascript">
!function(){var analytics=window.analytics=window.analytics||[];if(!analytics.initialize)if(analytics.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{analytics.invoked=!0;analytics.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","page","once","off","on"];analytics.factory=function(t){return function(){var e=Array.prototype.slice.call(arguments);e.unshift(t);analytics.push(e);return analytics}};for(var t=0;t<analytics.methods.length;t++){var e=analytics.methods[t];analytics[e]=analytics.factory(e)}analytics.load=function(t){var e=document.createElement("script");e.type="text/javascript";e.async=!0;e.src=("https:"===document.location.protocol?"https://":"http://")+"cdn.segment.com/analytics.js/v1/"+t+"/analytics.min.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n)};analytics.SNIPPET_VERSION="3.1.0";
analytics.load("8UDQfnf3cyFSTsM4YANnW5sXmgZVILbA");
analytics.page();
}}();

analytics.ready(function () {
    ga('require', 'linker');
    ga('linker:autoLink', ['scrapinghub.com', 'crawlera.com']);
});
</script>


</body>
</html>